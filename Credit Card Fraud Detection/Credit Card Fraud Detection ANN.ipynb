{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\wlpch\\Documents\\sab\\Credit Card Fraud Detection\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>2.926914</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>2.103355</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>-9.210340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>1.722784</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>3.091047</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>4.345947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>10.153902</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16       2.926914   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01       2.103355   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01      -9.210340   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       1.722784   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02       3.091047   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02       4.345947   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01      10.153902   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,:-1]\n",
    "y=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Amount']=np.log(data['Amount']+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>5.008099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.989578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>5.936639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>4.816242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>4.248354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28    Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  5.008099      0  \n",
       "1  0.125895 -0.008983  0.014724  0.989578      0  \n",
       "2 -0.139097 -0.055353 -0.059752  5.936639      0  \n",
       "3 -0.221929  0.062723  0.061458  4.816242      0  \n",
       "4  0.502292  0.219422  0.215153  4.248354      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "X_val=sc.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(units=256,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=256,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=32,kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "opt=Adam(learning_rate=0.0001)\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(optimizer=opt,loss=bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop=EarlyStopping(monitor='val_loss',mode='max',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.0028 - val_loss: 0.0031\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 1s 18ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.0020 - val_loss: 0.0030\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 2s 20ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 2s 21ms/step - loss: 0.0016 - val_loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "model_trained=model.fit(X_train,y_train,batch_size=2000,epochs=50,verbose=1,callbacks=early_stop,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85283,    13],\n",
       "       [   29,   118]], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85296\n",
      "           1       0.90      0.80      0.85       147\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.90      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 4.5242e-04 - val_loss: 4.5812e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 4.9740e-04 - val_loss: 3.8764e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 3.3969e-04 - val_loss: 2.2554e-04\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 2.5671e-04 - val_loss: 2.1277e-04\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 2.9972e-04 - val_loss: 1.9818e-04\n",
      "Epoch 6/20\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 1.7855e-04 - val_loss: 1.8390e-04\n",
      "Epoch 7/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 2.0419e-04 - val_loss: 1.8206e-04\n",
      "Epoch 8/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.5796e-04 - val_loss: 3.6618e-04\n",
      "Epoch 9/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 3.8101e-04 - val_loss: 6.0281e-04\n",
      "Accuracy in folds 1 : 0.9182266603884097\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 4.5477e-04 - val_loss: 2.0345e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 3s 20ms/step - loss: 2.2676e-04 - val_loss: 1.8837e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.6753e-04 - val_loss: 1.1761e-04\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 3s 19ms/step - loss: 1.3382e-04 - val_loss: 1.7186e-04\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.5361e-04 - val_loss: 1.2576e-04\n",
      "Accuracy in folds 2 : 0.9795566650971025\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 2.3071e-04 - val_loss: 1.8438e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 1.5230e-04 - val_loss: 7.9153e-05\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 3.1941e-04 - val_loss: 3.8721e-04\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.5560e-04 - val_loss: 1.7258e-04\n",
      "Accuracy in folds 3 : 0.9591133301942049\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.8311e-04 - val_loss: 1.4124e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.7176e-04 - val_loss: 4.5113e-05\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 2.8135e-04 - val_loss: 1.4881e-04\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.1323e-04 - val_loss: 2.1681e-04\n",
      "Accuracy in folds 4 : 0.9386699952913073\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 3s 21ms/step - loss: 1.7839e-04 - val_loss: 9.3408e-05\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 8.8765e-05 - val_loss: 6.6273e-05\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 6.4466e-05 - val_loss: 1.9702e-04\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.0460e-04 - val_loss: 9.2475e-05\n",
      "Accuracy in folds 5 : 0.9795566650971025\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.1318e-04 - val_loss: 1.4392e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 7.1663e-05 - val_loss: 2.9693e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 9.6894e-05 - val_loss: 2.5173e-04\n",
      "Accuracy in folds 6 : 0.9398944813759629\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 3s 19ms/step - loss: 9.0969e-05 - val_loss: 1.5429e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.1153e-04 - val_loss: 1.6487e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.1935e-04 - val_loss: 2.8363e-04\n",
      "Accuracy in folds 7 : 0.9198593085012838\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 3.0520e-04 - val_loss: 1.8459e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 1.5434e-04 - val_loss: 1.4750e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.2567e-04 - val_loss: 5.7318e-05\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 7.6732e-05 - val_loss: 2.1940e-04\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 2.6379e-04 - val_loss: 2.3160e-04\n",
      "Accuracy in folds 8 : 0.9386699915800445\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.5146e-04 - val_loss: 2.1816e-04\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 7.9023e-05 - val_loss: 2.4629e-04\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 8.4971e-05 - val_loss: 2.6968e-04\n",
      "Accuracy in folds 9 : 0.9591133277200297\n",
      "Epoch 1/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 2.1142e-04 - val_loss: 7.0808e-05\n",
      "Epoch 2/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 2.8497e-04 - val_loss: 5.2458e-05\n",
      "Epoch 3/20\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 1.6978e-04 - val_loss: 4.1298e-05\n",
      "Epoch 4/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 1.4003e-04 - val_loss: 5.5645e-05\n",
      "Epoch 5/20\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 9.3630e-05 - val_loss: 6.8104e-05\n",
      "Accuracy in folds 10 : 0.9795566638600148\n",
      "[0.9182266603884097, 0.9795566650971025, 0.9591133301942049, 0.9386699952913073, 0.9795566650971025, 0.9398944813759629, 0.9198593085012838, 0.9386699915800445, 0.9591133277200297, 0.9795566638600148]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "score_list = []\n",
    "folds = 1\n",
    "oof = np.zeros(len(X))\n",
    "\n",
    "\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "    \n",
    "    ann_model = model\n",
    "    ann_model.fit(x=X_train,y=y_train,\n",
    "                  validation_data=(X_val, y_val), epochs=20, batch_size=2000,callbacks=[early_stop])\n",
    "    \n",
    "    y_preds = ann_model.predict(X_val)\n",
    "    y_preds = np.where(y_preds > 0.5, 1,0)\n",
    "    \n",
    "    oof[test_index] = y_preds.reshape(-1,)\n",
    "    score = r2_score(y_val, oof[test_index])\n",
    "    score_list.append(score)\n",
    "    print(f\"Accuracy in folds {folds} : {score}\")\n",
    "    folds +=1\n",
    "    \n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9795566638600148\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284315\n",
      "           1       0.99      0.96      0.98       492\n",
      "\n",
      "    accuracy                           1.00    284807\n",
      "   macro avg       1.00      0.98      0.99    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[284312,      3],\n",
       "       [    21,    471]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y, oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote=SMOTE()\n",
    "X_smote,y_smote=smote.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 0.1399 - val_loss: 0.0403\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 0.0229 - val_loss: 0.0135\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 0.0097 - val_loss: 0.0067\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0050 - val_loss: 0.0056\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "Epoch 6/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 0.0023 - val_loss: 0.0037\n",
      "Epoch 7/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0016 - val_loss: 0.0023\n",
      "Epoch 8/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 9/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Accuracy in folds 1 : 0.9984524207300491\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 0.0013 - val_loss: 9.1310e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0015 - val_loss: 4.9126e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 9.2814e-04 - val_loss: 5.9916e-04\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Accuracy in folds 2 : 0.9991558658527541\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0010 - val_loss: 5.1618e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 7.3195e-04 - val_loss: 6.2476e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 8.0200e-04 - val_loss: 3.8661e-04\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 5.5747e-04 - val_loss: 3.8345e-04\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 8.9693e-04 - val_loss: 3.5421e-04\n",
      "Epoch 6/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 7/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 6.7742e-04 - val_loss: 3.5624e-04\n",
      "Accuracy in folds 3 : 0.999859310975459\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 5.7386e-04 - val_loss: 6.6926e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 6.3205e-04 - val_loss: 1.7426e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 6.8330e-04 - val_loss: 6.5035e-04\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 0.0015 - val_loss: 2.3194e-04\n",
      "Accuracy in folds 4 : 0.9999296554877295\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.7240e-04 - val_loss: 3.6365e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 2.7070e-04 - val_loss: 4.0600e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.4411e-04 - val_loss: 4.4576e-04\n",
      "Accuracy in folds 5 : 0.9996482774386475\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 4.8799e-04 - val_loss: 2.7032e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 2.7156e-04 - val_loss: 3.5307e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 7.0750e-04 - val_loss: 4.6849e-04\n",
      "Accuracy in folds 6 : 0.9996482774386475\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 20ms/step - loss: 5.4916e-04 - val_loss: 1.1223e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 4s 16ms/step - loss: 0.0016 - val_loss: 1.2687e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 3.1568e-04 - val_loss: 1.7860e-04\n",
      "Accuracy in folds 7 : 1.0\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 2.7055e-04 - val_loss: 3.3806e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 2.3325e-04 - val_loss: 3.7016e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.0422e-04 - val_loss: 3.3397e-04\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 3.5237e-04 - val_loss: 5.2106e-04\n",
      "Epoch 5/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 2.9273e-04 - val_loss: 3.9257e-04\n",
      "Accuracy in folds 8 : 0.999718621950918\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 18ms/step - loss: 8.3395e-04 - val_loss: 5.9909e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 2.5993e-04 - val_loss: 3.1252e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 2.3978e-04 - val_loss: 5.3162e-04\n",
      "Epoch 4/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 5.5104e-04 - val_loss: 4.6022e-04\n",
      "Accuracy in folds 9 : 0.9996482774386475\n",
      "Epoch 1/20\n",
      "256/256 [==============================] - 5s 19ms/step - loss: 4.5077e-04 - val_loss: 2.9940e-04\n",
      "Epoch 2/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 3.6176e-04 - val_loss: 8.5411e-04\n",
      "Epoch 3/20\n",
      "256/256 [==============================] - 4s 17ms/step - loss: 2.4290e-04 - val_loss: 0.0010\n",
      "Accuracy in folds 10 : 0.9984524207300491\n",
      "[0.9984524207300491, 0.9991558658527541, 0.999859310975459, 0.9999296554877295, 0.9996482774386475, 0.9996482774386475, 1.0, 0.999718621950918, 0.9996482774386475, 0.9984524207300491]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "score_list = []\n",
    "folds = 1\n",
    "oof = np.zeros(len(X_smote))\n",
    "\n",
    "\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X_smote,y_smote):\n",
    "    X_train, X_val = X_smote.iloc[train_index], X_smote.iloc[test_index]\n",
    "    y_train, y_val = y_smote.iloc[train_index], y_smote.iloc[test_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "    \n",
    "    ann_model = model\n",
    "    ann_model.fit(x=X_train,y=y_train,\n",
    "                  validation_data=(X_val, y_val), epochs=20, batch_size=2000,callbacks=[early_stop])\n",
    "    \n",
    "    y_preds = ann_model.predict(X_val)\n",
    "    y_preds = np.where(y_preds > 0.5, 1,0)\n",
    "    \n",
    "    oof[test_index] = y_preds.reshape(-1,)\n",
    "    score = r2_score(y_val, oof[test_index])\n",
    "    score_list.append(score)\n",
    "    print(f\"Accuracy in folds {folds} : {score}\")\n",
    "    folds +=1\n",
    "    \n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9984524207300491\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284315\n",
      "           1       1.00      1.00      1.00    284315\n",
      "\n",
      "    accuracy                           1.00    568630\n",
      "   macro avg       1.00      1.00      1.00    568630\n",
      "weighted avg       1.00      1.00      1.00    568630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_smote, oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[284259,     56],\n",
       "       [    22, 284293]], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_smote, oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy='not minority')\n",
    "under = RandomUnderSampler()\n",
    "\n",
    "steps = [(\"over\", over),\n",
    "         (\"under\", under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X_pip, y_pip = pipeline.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 4.5008e-09 - val_loss: 5.3296e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 4.4036e-09 - val_loss: 5.2505e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 4.3287e-09 - val_loss: 5.1851e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 4.2271e-09 - val_loss: 5.0637e-09\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 4.1407e-09 - val_loss: 4.9852e-09\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 4.0343e-09 - val_loss: 4.8646e-09\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 3.9372e-09 - val_loss: 4.7761e-09\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 3.8549e-09 - val_loss: 4.6996e-09\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 3.7714e-09 - val_loss: 4.6045e-09\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 3.6832e-09 - val_loss: 4.5065e-09\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 3.6011e-09 - val_loss: 4.4270e-09\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.5114e-09 - val_loss: 4.3370e-09\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.4323e-09 - val_loss: 4.2659e-09\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.3604e-09 - val_loss: 4.1815e-09\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.2826e-09 - val_loss: 4.1071e-09\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.2090e-09 - val_loss: 4.0229e-09\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.1335e-09 - val_loss: 3.9551e-09\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.0675e-09 - val_loss: 3.8760e-09\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.9970e-09 - val_loss: 3.8190e-09\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.9395e-09 - val_loss: 3.7693e-09\n",
      "Accuracy in folds 1 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 3.4376e-09 - val_loss: 1.0898e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.3030e-09 - val_loss: 1.0529e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.1810e-09 - val_loss: 1.0189e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.1058e-09 - val_loss: 9.9549e-10\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 3.0223e-09 - val_loss: 9.7160e-10\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.9443e-09 - val_loss: 9.4585e-10\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.8809e-09 - val_loss: 9.2736e-10\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.8283e-09 - val_loss: 9.0982e-10\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 2.7718e-09 - val_loss: 8.9388e-10\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 2.7195e-09 - val_loss: 8.7637e-10\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 2.6536e-09 - val_loss: 8.5432e-10\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.5955e-09 - val_loss: 8.3880e-10\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.5494e-09 - val_loss: 8.2323e-10\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.4982e-09 - val_loss: 8.0881e-10\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.4273e-09 - val_loss: 7.8540e-10\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.3761e-09 - val_loss: 7.7162e-10\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.3263e-09 - val_loss: 7.5788e-10\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.2943e-09 - val_loss: 7.4649e-10\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.2533e-09 - val_loss: 7.3338e-10\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 2.2200e-09 - val_loss: 7.2088e-10\n",
      "Accuracy in folds 2 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.9884e-09 - val_loss: 1.0211e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.9330e-09 - val_loss: 1.0112e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.8893e-09 - val_loss: 1.0013e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.8482e-09 - val_loss: 9.9077e-10\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.8072e-09 - val_loss: 9.7840e-10\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.7717e-09 - val_loss: 9.7089e-10\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.7384e-09 - val_loss: 9.6234e-10\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.7038e-09 - val_loss: 9.4784e-10\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.6713e-09 - val_loss: 9.5034e-10\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.6370e-09 - val_loss: 9.3941e-10\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.6074e-09 - val_loss: 9.4321e-10\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.5901e-09 - val_loss: 9.4679e-10\n",
      "Accuracy in folds 3 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.6646e-09 - val_loss: 1.9874e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.6213e-09 - val_loss: 1.9597e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.5776e-09 - val_loss: 1.9371e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.5431e-09 - val_loss: 1.9202e-09\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.5133e-09 - val_loss: 1.9000e-09\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.4862e-09 - val_loss: 1.8904e-09\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.4761e-09 - val_loss: 1.8719e-09\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.4456e-09 - val_loss: 1.8578e-09\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.4291e-09 - val_loss: 1.8456e-09\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 1.4142e-09 - val_loss: 1.8296e-09\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.3921e-09 - val_loss: 1.8158e-09\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.3730e-09 - val_loss: 1.8025e-09\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.3569e-09 - val_loss: 1.7860e-09\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.3158e-09 - val_loss: 1.7459e-09\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.2991e-09 - val_loss: 1.7376e-09\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2803e-09 - val_loss: 1.7232e-09\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.2684e-09 - val_loss: 1.7178e-09\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2583e-09 - val_loss: 1.7102e-09\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.2455e-09 - val_loss: 1.7028e-09\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2322e-09 - val_loss: 1.6852e-09\n",
      "Accuracy in folds 4 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.2558e-09 - val_loss: 1.7533e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.2381e-09 - val_loss: 1.7328e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2179e-09 - val_loss: 1.7067e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.2041e-09 - val_loss: 1.6845e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.1836e-09 - val_loss: 1.6617e-09\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.1696e-09 - val_loss: 1.6395e-09\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.1525e-09 - val_loss: 1.6246e-09\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1393e-09 - val_loss: 1.6057e-09\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1273e-09 - val_loss: 1.5906e-09\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1206e-09 - val_loss: 1.5732e-09\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1097e-09 - val_loss: 1.5645e-09\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1078e-09 - val_loss: 1.5508e-09\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1019e-09 - val_loss: 1.5394e-09\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0915e-09 - val_loss: 1.5296e-09\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0904e-09 - val_loss: 1.5194e-09\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0869e-09 - val_loss: 1.5133e-09\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0873e-09 - val_loss: 1.5002e-09\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0654e-09 - val_loss: 1.4878e-09\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0652e-09 - val_loss: 1.4847e-09\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0664e-09 - val_loss: 1.4782e-09\n",
      "Accuracy in folds 5 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.1888e-09 - val_loss: 1.6375e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1508e-09 - val_loss: 1.6293e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1296e-09 - val_loss: 1.6320e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1315e-09 - val_loss: 1.6510e-09\n",
      "Accuracy in folds 6 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.3494e-09 - val_loss: 6.6362e-10\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.2105e-09 - val_loss: 6.1101e-10\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.1375e-09 - val_loss: 5.7710e-10\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0878e-09 - val_loss: 5.4963e-10\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0431e-09 - val_loss: 5.4716e-10\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0468e-09 - val_loss: 5.4588e-10\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0460e-09 - val_loss: 5.5063e-10\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 1.0526e-09 - val_loss: 5.5507e-10\n",
      "Accuracy in folds 7 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 1.0102e-09 - val_loss: 4.9742e-10\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.8861e-10 - val_loss: 4.9393e-10\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.7479e-10 - val_loss: 4.9125e-10\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.5992e-10 - val_loss: 4.8873e-10\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.5213e-10 - val_loss: 4.8687e-10\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.4805e-10 - val_loss: 4.8760e-10\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.5214e-10 - val_loss: 4.8817e-10\n",
      "Accuracy in folds 8 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 1.4858e-09 - val_loss: 1.0252e-09\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.4269e-10 - val_loss: 1.0182e-09\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.4511e-10 - val_loss: 1.0278e-09\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.3488e-10 - val_loss: 1.0178e-09\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.3294e-10 - val_loss: 1.0291e-09\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.4247e-10 - val_loss: 1.0373e-09\n",
      "Accuracy in folds 9 : 1.0\n",
      "Epoch 1/20\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 8.4498e-10 - val_loss: 7.5731e-10\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.2742e-10 - val_loss: 7.5475e-10\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.3012e-10 - val_loss: 7.5702e-10\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.4088e-10 - val_loss: 7.6251e-10\n",
      "Accuracy in folds 10 : 1.0\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "score_list = []\n",
    "folds = 1\n",
    "ooff = np.zeros(len(X_pip))\n",
    "\n",
    "\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X_pip,y_pip):\n",
    "    X_train, X_val = X_pip.iloc[train_index], X_pip.iloc[test_index]\n",
    "    y_train, y_val = y_pip.iloc[train_index], y_pip.iloc[test_index]\n",
    "\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "    \n",
    "    ann_model = model\n",
    "    ann_model.fit(x=X_train,y=y_train,\n",
    "                  validation_data=(X_val, y_val), epochs=20, batch_size=20,callbacks=[early_stop])\n",
    "    \n",
    "    y_preds = ann_model.predict(X_val)\n",
    "    y_preds = np.where(y_preds > 0.5, 1,0)\n",
    "    \n",
    "    ooff[test_index] = y_preds.reshape(-1,)\n",
    "    score = r2_score(y_val, ooff[test_index])\n",
    "    score_list.append(score)\n",
    "    print(f\"Accuracy in folds {folds} : {score}\")\n",
    "    folds +=1\n",
    "    \n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       492\n",
      "           1       1.00      1.00      1.00       492\n",
      "\n",
      "    accuracy                           1.00       984\n",
      "   macro avg       1.00      1.00      1.00       984\n",
      "weighted avg       1.00      1.00      1.00       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pip, ooff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[492,   0],\n",
       "       [  0, 492]], dtype=int64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pip, ooff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding best model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=512,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner.search(X_T,y_t,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
